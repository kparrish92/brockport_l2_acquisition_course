---
title   : 'Data Science for Linguists'
subtitle: 'Distributions'
author: "Joseph V. Casillas, PhD"
institute: "Rutgers University<mybr>Spring 2025<br>Last update: `r Sys.Date()`"
---

```{r}
#| label: load-helpers
#| echo: false 
#| message: false 
#| warning: false
#| cache: false
source(here::here("assets", "scripts", "helpers.R"))
```

# Statistical thinking {.transition}

---

## Statistical thinking <mybr>[A necessary evil]{style="font-size: 0.7em; color: #666666;"} {data-menu-title="A necessary evil"}

::: {.incremental style="font-size: 0.9em;"}

- Mastering quantitative methods is an "[...] increasingly vital component of linguistic training" [@johnson2011quantitative]. 

- Main roadblock = lack of resources

- Most programs don't offer this training. Students get it elsewhere (or not at all)

- Not a lot of good texts made specifically for linguistics
  - "Linguists learn to do stats on the street."
  - "If you want to be a phonetician today, you really have to train to be a programmer."

:::

---

## Statistical thinking <mybr>[Big picture]{style="font-size: 0.7em; color: #666666;"} {data-menu-title="Big picture"}

- It's all guessing. We use stats and hypothesis testing to assess our guesses

> "All models are wrong, but some are useful"  
- George Box

. . .

- A primary goals of this class: teach you to build useful models and to understand how to assess their usefulness

---

## Statistical thinking <mybr>[An aside]{style="font-size: 0.7em; color: #666666;"} {data-menu-title="An aside"}

- Statistics is hard

- You are learning 

- You will make mistakes

- You should come away with a better understanding of the big picture 

- Ideally this would be the first class of many you take on the subject

---

## Statistical thinking <br>[Goals [according to @johnson2011quantitative]]{style="font-size: 0.7em; color: #666666;"} {data-menu-title="Goals"}

1. Data reduction

2. Inference

3. Discovery of relationships

4. Exploration

. . .

Really?

---

## {background-image="../assets/img/workflow/datascience_workflow.png" background-size=contain data-menu-title="EDA or CDA?"}

::: columns
::: {.column width="40%"}
:::

::: {.column width="33%"}
:::{.emph .p-font style="font-size: 1.75em;"}
EDA or CDA?
:::
:::

:::

---

::: {.r-stack .emph .p-font style="font-size: 1.45em;"}
Exploratory data analysis
:::
::: {.r-stack .emph .p-font style="font-size: 1.45em;"}
vs.
:::
::: {.r-stack .emph .p-font style="font-size: 1.45em;"}
Confirmatory data analysis
:::

<mybr>

::: {.p-font style="color: #666666;"}
[EDA]{style="color: #497dd7;"}: used to get to know your data, to generate hypotheses  
[CDA]{.emph}: used ONLY to test hypotheses
:::

::: {style="font-size: 0.7em;"}
- Issues
  - data dredging, p-hacking
  - Many scientists do EDA first, and establish their hypotheses a posteriori (once you have already found "significant" differences)
- This is pseudoscience!
:::

<mybr>

::: {.r-stack style="font-size: 0.7em;"}
Both [EDA]{style="color: #497dd7;" .p-font} and [CDA]{.emph .p-font} (done correctly) are a necessary part of science
:::




# Observations {.transition}

## {background-image="./index_files/img/observation1_1.png" background-size="contain" background-position="center" visibility="uncounted"}

::: {.emph .p-font style="font-size: 1.75em;"}
What is an observation anyway?
:::

::: notes
- Observation: Using the senses for some motivated purpose
- Senses not always reliable and don't necessarily perfectly represent reality: 
  - Sensory observations are already a step removed from reality because they 
  are first passed through our perceptions and reinterpreted before we actually
  “make sense” of them
  - In other words, our raw sensory input is not what we perceive

- We have many evolved adaptive mechanisms for deciphering sensory input:
  - First, we have a sensation
  - Then, perception reconstructs it into a meaningful message in our brain
:::

---

## {background-image="./index_files/img/observation1_2.png" background-size="contain" background-position="center" visibility="uncounted"}

::: {.emph .p-font style="font-size: 1.75em;"}
What is an observation anyway?
:::

---

## {background-image="./index_files/img/observation1_3.png" background-size="contain" background-position="center" visibility="uncounted"}

::: {.emph .p-font style="font-size: 1.75em;"}
What is an observation anyway?
:::

---

## {background-image="./index_files/img/observation1_4.png" background-size="contain" background-position="center" visibility="uncounted"}

::: {.emph .p-font style="font-size: 1.75em;"}
What is an observation anyway?
:::

---

## {background-image="./index_files/img/observation2_1.png" background-size="contain" background-position="center" visibility="uncounted"}

::: {.emph .p-font style="font-size: 1.75em;"}
What is an observation anyway?
:::

::: notes
- Where do we get these numbers? 1) Observation 2) process of measurement
- Measurement = processes of assigning a number to an observation
- Why? Because in most experimental research every observation must be turned into a number 
- We then use statistics to analyze our numbers

- Rules of Measurement
  - numbers are not arbitrary
  - there is a meaningful and selective process
  - must be reliable and replicable (Inter-Rater, Inter-Item, Test-Retest)
  - must be valid (external validity)
- In sum: Not all numbers are scientific data: must be reliable, valid measurements based on empirical observations
:::

---

## {background-image="./index_files/img/observation2_2.png" background-size="contain" background-position="center" visibility="uncounted"}

::: {.emph .p-font style="font-size: 1.75em;"}
What is an observation anyway?
:::

---

## {background-image="./index_files/img/observation2_3.png" background-size="contain" background-position="center" visibility="uncounted"}

::: {.emph .p-font style="font-size: 1.75em;"}
What is an observation anyway?
:::

---

## {background-image="./index_files/img/observation_sum.png" background-size="1200px" background-position="50% 95%" visibility="uncounted"}

::: {.emph .p-font style="font-size: 1.75em;"}
What is an observation anyway?
:::

::: notes
- There is a lot of room for bias and error in this multi-stage process!
- Numbers don't "speak for themselves"
- Our brain's process the number, we speak for them
- We interpret the numbers based what we **think** they represent
- All our interpretations are subject to error and there are many opportunities to make errors in the process
- So how do we know that our numbers aren't too far removed from reality?
:::

---

## {visibility="uncounted"}

::: {.emph .p-font style="font-size: 1.75em;"}
What is an observation anyway?
:::

::: {.p-font style="font-size: 1.25em; color: #666666;"}
@johnson2011quantitative discusses 4 types...
:::

::: {style="font-size: 0.65em"}
1. Nominal
2. Ordinal
3. Interval
4. Ratio
:::

. . .

::: {.p-font style="font-size: 0.9em; color: #666666;"}
The type of observations you deal with are related to your area of research and your questions.
:::

. . .

::: {.p-font style="font-size: 0.9em; color: #666666;"}
The type of observations you analyze will determine the statistical methods you use to answer your questions. 
:::

. . .

::: {.p-font style="font-size: 0.9em; color: #666666;"}
Different types of observations are distributed in different ways.
:::





# Distributions {.transition}

## {background-image="https://www.jvcasillas.com/media/rstats/memes/rstats_pop_v_sample.png" background-size="contain"}

---

## Distributions<mybr>[The basics]{style="font-size: 0.7em; color: #666666;"} {.smaller data-menu-title="The basics"}

::: columns
::: {.column width="45%"}
- If we collect a series of observations we have a **sample** of data

- We can create a `histogram` by selecting a bin size and counting the 
frequency of the values in our sample that fall inside the bin

- A histogram shows us the frequency distribution of our sample

- For example, the histogram to the right shows that 
the value `3` appears in this sample over 200 times
:::

::: {.column width="55%"}
```{r}
#| label: normal_distribution1
#| cache: false
#| echo: false
#| fig-height: 5
#| fig-width: 6
#| fig-asp: 0.9
#| fig-align: 'right'
# 1000 random numbers with a mean of 5, sd of 2
my_normal_x <- rnorm(1000, mean = 3.5, sd = 1.5)

# Histogram of data
p1 <- tibble(x = my_normal_x) |> 
  ggplot() +
  aes(x = x)

p1 + geom_histogram(binwidth = 1, fill = 'thistle', color = 'white') +
  scale_x_continuous(breaks = -1:8, labels = -1:8) + 
  ds4ling_theme()
```
:::
:::

---

## Distributions<br>[Describing the sample]{style="font-size: 0.7em; color: #666666;"} {data-menu-title="Describing the sample"}

We can describe the characteristics of our sample to see how the values are 
distributed. 

For this we use...

1. Measures of central tendency (mid-point)

2. Measures of dispersion (around mid-point)

---

## Distributions<br>[Describing the sample]{style="font-size: 0.7em; color: #666666;"} {data-menu-title="Measures of central tendency"}

<mybr>

[Measures of central tendency (mid-point)]{style="font-size: 0.7em; color: black;" .p-font}

```{r}
#| label: trivial-sample
#| echo: true
# Create a vector of numbers
my_data <- c(2, 5, 8)
```

. . .

::: columns
::: {.column style="font-size: 0.7em;"}

<br>

- [mean]{style="color: #497dd7;" .p-font}: arithmetic average, least squares estimate of central tendency

- [median]{style="color: #497dd7;" .p-font}: value located in the middle

- [mode]{style="color: #497dd7;" .p-font}: most frequently occurring value
:::

::: {.column .fragment}

<br>

```{r}
#| label: avg-ex
#| echo: true

(2 + 5 + 8) / 3

mean(my_data)
median(my_data)
```
:::
:::

---

## Distributions<br>[Describing the sample]{style="font-size: 0.7em; color: #666666;"} {data-menu-title="Measures of dispersion"}

::: columns
::: {.column style="font-size: 0.7em;" .incremental}

[Measures of dispersion (around mid-point)]{style="font-size: 1em; color: black;" .p-font}

```{r}
#| label: print-ex
#| echo: false
my_data 
```

- [range]{style="color: #497dd7;" .p-font}: difference between highest and lowest value

<mybr>

- [variance]{style="color: #497dd7;" .p-font}: subtract mean from each value, sum the squares and divide by n - 1

<mybr>

- [standard deviation]{style="color: #497dd7;" .p-font}: square root of the variance
:::

::: {.column .fragment}
```{r}
#| label: range
#| echo: true
#| results: 'hold'
max(my_data) - min(my_data)
range(my_data)
```

<mybr>

```{r}
#| label: variance
#| echo: true
# 2 - 5 = -3^2 = 9
# 5 - 5 =  0^2 = 0
# 8 - 5 =  3^2 = 9
#              = 18 / 2
# Variance = 9
var(my_data)
```

<mybr>

```{r}
#| label: sd
#| echo: true
# SD (square root of 9) = 3
sd(my_data)
```
:::
:::

---

## Distributions<br>[More about distributions]{style="font-size: 0.7em; color: #666666;"} {data-menu-title="More about distributions"}

::: columns
::: {.column style="font-size: 0.85em;"}
- There are many <u>families</u> of distributions[^dists], each of which have important characteristics. 

- The most important for us (for now) is the normal distribution. 

- It is easy to recognize because it looks like a bell. 
:::

::: {.column}
```{r}
#| label: normal_distribution2 
#| echo: false
#| fig-asp: 0.9
#| fig-height: 5
#| fig-align: 'right'

p1 + geom_histogram(binwidth = 1, fill = 'thistle', color = 'white') +
  scale_x_continuous(breaks = -1:8, labels = -1:8) + 
  ds4ling_theme()
```
:::
:::

[^dists]: i.e., normal distribution, t-distribution, F-distribution, 
χ-distribution, etc.

::: notes

```
# 1000 random numbers with a mean of 5, sd of 1.5
my_normal_x <- rnorm(1000, mean = 5, sd = 1.5)

# Histogram of data
p1 <- tibble(x = my_normal_x) |> 
  ggplot() + 
  aes(x = x) + 
  geom_histogram(
    aes(y = after_stat(density)), 
    binwidth = 1, fill = 'thistle', color = 'white'
  ) +
  geom_density(bw = 0.75) + 
  scale_x_continuous(breaks = 1:10, labels = 1:10) +
  coord_cartesian(xlim = c(0, 10)) + 
  ds4ling_theme()

print(p1)
```
:::


---

## Distributions<br>[The normal distribution]{style="font-size: 0.7em; color: #666666;"} {.smaller data-menu-title="The normal distribution"}

- If we decrease the bin size in our histogram (the range of observations used 
for each vertical bar) we can see the distribution looks more bell-like

```{r}
#| label: normal-distributions3
#| fig-asp: 0.35

nd_0 <- function(bw = 0.66) {
  list(
    geom_histogram(binwidth = bw, fill = 'thistle', color = 'white'), 
    labs(y = NULL, x = NULL), 
    ds4ling_theme(), 
    theme(axis.text = element_blank())
  )
}

nd_1 <- p1 + nd_0(bw = 0.88)
nd_2 <- p1 + nd_0(bw = 0.66)
nd_3 <- p1 + nd_0(bw = 0.30)
nd_4 <- p1 +
  geom_histogram(
    aes(y = after_stat(density)), 
    binwidth = 0.15, fill = 'thistle', color = 'white'
  ) +
  geom_density(bw = 0.75, color = 'black', linewidth = 1) + 
  labs(y = NULL, x = NULL) + 
  ds4ling_theme() + 
  theme(axis.text = element_blank())

nd_1 | nd_2 | nd_3 | nd_4
```

---

[Distributions]{style="font-size: 1.75em;" .emph .p-font}

[The basics]{style="font-size: 1.2em; color: #666666;" .p-font} 

::: columns
::: {.column style="font-size: 0.8em;"}
- If we continue decreasing the bin size we can also see that the rectangular 
shape of the vertical bars becomes a curved line

- We can do this to the point that the bin sizes are decreased to the limit at 
zero

- The formula that derives this line is called a probability density function. 
:::
::: {.column}
```{r}
#| label: normal-distributions4
#| fig-height: 5
#| fig-asp: 0.8
#| fig-align: 'right'
p1 + geom_histogram(
    aes(y = after_stat(density)), 
    binwidth = 1, fill = 'thistle', color = 'white' 
  ) +
  geom_density(bw = 0.75, color = 'black', linewidth = 1) + 
  ds4ling_theme()
```
:::
:::

---

## {.center}

::: {.p-font .emph style="font-size: 1.3em;"}
<center>
We can calculate the probability of any set of observations
by finding the area under any portion of the curve
</center>
:::

. . .

<br>

<center>
[This will be important for hypothesis testing later...]{style="font-size: 0.75em;"}
</center>

---

## Distributions<br>[Less-than-normal data]{style="font-size: 0.7em; color: #666666;"} {data-menu-title="Less-than-normal data"}

::: columns
::: {.column}
- Likewise it is easy to notice when a distribution is not normal. 
- We can end up with non-normal data for many reasons. 
- Sometimes it depends on the nature of what we are measuring.
:::

::: {.column}
```{r}
#| label: uniform_distribution
#| fig-height: 5
#| fig-asp: 0.8
#| fig-align: 'right'
# 100 random uniform numbers from 1 to 6 (a die)
my_uniform_x <- sample(1:6, size = 1000, replace = TRUE) 

# Histogram of data
p2 <- tibble(x = my_uniform_x) |> 
  ggplot() + 
  aes(x = x) + 
  geom_histogram(
    aes(y = after_stat(density)), 
    binwidth = 1, fill = 'thistle', color = 'white'
  ) +
  stat_function(fun = dnorm, args = list(mean = 3.5, sd = 1.7)) + 
  scale_x_continuous(breaks = 1:6, labels = 1:6, limits = c(-2, 9), 
    oob = function(x, limits) x) + 
  annotate("text", x = -1, y = 0.20, parse = T, size = 8,
    label = paste0("mu == 3.5")) + 
  annotate("text", x = -1, y = 0.18, parse = T, size = 8,
    label = paste0("sigma == 1.7")) + 
  ds4ling_theme()

p2
```
:::
:::

::: notes
```
# 100 random uniform numbers from 1 to 6 (a die)
my_uniform_x <- sample(1:6, size = 1000, replace = TRUE) 

# Histogram of data
p2 <- tibble(x = my_uniform_x) %>% 
  ggplot() + 
  aes(x = x) + 
  geom_histogram(binwidth = 1, fill = 'grey90', 
    color = 'black', aes(y = ..density..)) + 
  stat_function(fun = dnorm, args = list(mean = 3.5, sd = 1.7)) + 
  scale_x_continuous(limits = c(-2, 9), breaks = 1:6, labels = 1:6) + 
  annotate("text", x = -1, y = 0.20, parse = T, 
    label = paste0("mu == 3.5")) + 
  annotate("text", x = -1, y = 0.19, parse = T, 
    label = paste0("sigma == 1.7")) + 
  ds4ling_theme()
```
:::

---

[Distributions]{style="font-size: 1.75em;" .emph .p-font}

[Less-than-normal data]{style="font-size: 1.2em; color: #666666;" .p-font} 

::: {style="font-size: 0.65em;"}
- because of the nature of the data generating process
- because of outliers
- because of other underlying population differences
:::

```{r}
#| label: non_normal_distributions
#| fig-asp: 0.35
#| fig-align: 'right'
my_skewed_x <- rsnorm(1000, mean = 2.5, sd = 1.5, xi = 3.5)

p3 <- tibble(x = my_skewed_x) |>
  ggplot() +
  aes(x = x) + 
  geom_histogram(aes(y = after_stat(density)), 
    binwidth = 0.5, size = 0.75, fill = 'darkgrey', color = 'white') +
  geom_density(bw = 0.5) + 
  scale_x_continuous(
    limits = c(-3, 11), breaks = c(0, 4, 8), labels = c(0, 4, 8), 
    oob = function(x, limits) x) +
  ds4ling_theme() + 
  theme(
    axis.text.y = element_blank(), axis.title.y = element_blank(), 
    axis.text.x = element_blank(), axis.title.x = element_blank()
  )

# Vector with outliers
my_outlier_x <- c(
  rnorm(n = 900, mean = 3, sd = 1.25), 
  rnorm(n = 100, mean = 8.5, sd = 0.5)
  )

p4 <- tibble(x = my_outlier_x) |> 
  ggplot() + 
  aes(x = x) + 
  geom_histogram(aes(y = after_stat(density)), 
    binwidth = 0.5, size = 0.75, fill = 'thistle', color = 'white') +
  geom_density(bw = 0.5) + 
  scale_x_continuous(limits = c(-3, 11), breaks = c(0, 4, 8), 
    labels = c(0, 4, 8), oob = function(x, limits) x) + 
  ds4ling_theme() + 
  theme(
    axis.text.y = element_blank(), axis.title.y = element_blank(), 
    axis.text.x = element_blank(), axis.title.x = element_blank()
  )

p3 | p4
```


::: notes
```
# Vector with outliers
my_skewed_x <- c(rnorm(n = 900, mean = 3.5, sd = 1.5), 
                 rnorm(n = 100, mean = 10.5, sd = 1.5))

p3 <- tibble(x = my_skewed_x) %>% 
  ggplot(., aes(x = x)) + 
    geom_histogram(binwidth = 1, fill = 'grey90', color = 'black', 
                   aes(y = ..density..)) + 
    ds4ling_theme()

print(p3)
```
:::

---

## {.center}

```{r}
#| label: p123
#| fig-height: 8
#| fig-width: 14
#| fig-align: 'center'
# combine vectors to plot all three distributions on top of each other 
combined_dists <- 
  tibble(
    normal = my_normal_x, 
    uniform = my_uniform_x, 
    skewed = my_skewed_x, 
    outliers = my_outlier_x
  ) |> 
  pivot_longer(
    cols = everything(), 
    names_to = "type", 
    values_to = "val"
  )

# Calculate some descriptives
dist_desc <- combined_dists |> 
  group_by(type) |> 
  summarize(
    mean = round(mean(val), 2), 
    median = round(median(val), 2), 
    sd = round(sd(val), 2), .groups = "drop"
  )

p5 <- combined_dists |>
  ggplot() +
  aes(x = val, y = type, fill = type, height = after_stat(density)) + 
  geom_density_ridges(scale = 1.8, stat = 'density') + 
  scale_fill_brewer("Blues", guide = "none") + 
  geom_vline(xintercept = 3.5, lty = 3, color = 'grey60') + 
  ds4ling_theme() + 
  theme(axis.text.y = element_text(vjust = 0))

p5
```

---

## Distributions <mybr>[Is my data normal]{style="font-size: 0.7em; color: #666666;"} {data-menu-title="Is my data normal"}

[QQ plots]{.p-font style="font-size: 0.65em;"}

::: columns
::: {.column style="font-size: 0.65em;"}
- A good way to check if our data is normal 
- A qq-plot = plot quantile scores from data and quantile scores predicted 
by normal curve.
- If data come from the same distribution, the points follow the reference line.
  - We can check the correlation between a vector of data and the theoretical 
  quantiles. 
  - Normal data should be highly correlated with theoretical quantiles from the 
  normal distribution (more on correlation next week).
:::

::: {.column}
```{r}
#| label: qqplots1
#| fig-asp: 0.8
#| fig-align: 'right'
qq1 <- gg_qqplot(my_normal_x)
qq2 <- gg_qqplot(my_uniform_x)
qq3 <- gg_qqplot(my_skewed_x)
qq4 <- gg_qqplot(my_outlier_x)

p1_qq1 <- p1 + 
  geom_histogram(binwidth = 1, fill = 'thistle', color = 'white') +
  scale_x_continuous(breaks = -1:8, labels = -1:8) + 
  ds4ling_theme() 

p1_qq1 / qq1 
```
:::
:::

---

## {.center}

```{r}
#| label: qqplots234
#| fig-width: 14
#| fig-height: 8.5
# Plot all samples and qq plots
p2a <- p2 + theme(
    axis.text.y = element_blank(), axis.title.y = element_blank(), 
    axis.text.x = element_blank(), axis.title.x = element_blank()
  )

(p2a | p3 | p4) / (qq2 | qq3 | qq4)
```

---

## Distributions <mybr>[Empirical rule]{style="font-size: 0.7em; color: #666666;"} {.smaller data-menu-title="Empirical rule"}

- There is something else special about the normal distribution. 
- It has a useful property called the [**empirical rule**]{.emph}[^er]

<br>

For data which follow a normal distribution:

::: columns
::: {.column width="70%"}
- 68% of the data fall within 1σ of µ
- 95% of the data fall within 2σ of µ
- 99.7% of the data fall within 3σ of µ 
:::

::: {.column width="30%"}
- [**µ**]{.emph} (mew): pop. mean
- [**σ**]{.emph} (sigma): pop. sd
- [**x̄**]{.emph} (x-bar): sample mean
:::
:::

[^er]: Also called the 68-95-99.7 rule

---

## Distributions<mybr>[Standardizing]{style="font-size: 0.7em; color: #666666;"} {.smaller data-menu-title="Standardizing"}

- We can take samples of data and [**standardize**]{.emph} them--convert them 
to z-scores--to put them on the same scale (this will be a handy technique 
later on)
- [z-score]{.p-font style="color: #497dd7;"}: A z-score (or standard score) represents the number of 
standard deviations a given value x falls from the mean, μ. 

. . .

[An example]{.p-font style="font-size: 1.2em; color: #666666;"}

<mybr>

```{r}
#| label: create-vectors
#| echo: true
vec1 <- c(2, 5, 8)
vec2 <- c(22, 55, 88)
```

<br>

::: columns
::: {.column}
```{r}
#| label: mean-sd-vec1
#| echo: true
#| results: 'hold'
mean(vec1)
sd(vec1)
```
:::

::: {.column}

```{r}
#| label: mean-sd-vec2
#| echo: true
#| results: 'hold'
mean(vec2)
sd(vec2)
```
:::

:::

---

## Distributions<mybr>[Convert to [z-score]{.emph}]{style="font-size: 0.7em; color: #666666;"} {data-menu-title="Z-scores"}

::: {.r-fit-text}
[Subtract the mean]{style="color: orange;"} from [each value]{style="color: green;"} and divide by the [standard deviation]{style="color: blue;"}
:::

$$\color{red}{z} = \frac{\color{green}{x} - \color{orange}{\mu}}{\color{blue}{\sigma}}$$

. . .

<mybr>

::: columns
::: {.column}
### vec1

```
2 - 5 = -3 / 3 = -1
5 - 5 =  0 / 3 =  0
8 - 5 =  3 / 3 =  1
```
:::

::: {.column}
### vec2

```
22 -  55 = -33 / 33 = -1
55 -  55 =   0 / 33 =  0
88 -  55 =  33 / 33 =  1
```
:::
:::

---

[Distributions]{.emph .p-font style="font-size: 1.75em;"} 

[Convert to [z-score]{.emph}]{.p-font style="font-size: 1.2em; color: #666666;"} 

::: {.r-fit-text}
[Subtract the mean]{style="color: orange;"} from [each value]{style="color: green;"} and divide by the [standard deviation]{style="color: blue;"}
:::

$$\color{red}{z} = \frac{\color{green}{x} - \color{orange}{\mu}}{\color{blue}{\sigma}}$$

. . .

<mybr>

::: columns
::: {.column .small-code}

```{r}
#| label: std-vec1-manual
#| echo: true
vec1_std <- (vec1 - mean(vec1)) / sd(vec1)
vec1_std

mean(vec1_std); sd(vec1_std)
```

:::

::: {.column .small-code}
```{r}
#| label: std-vec2-manual
#| echo: true
vec2_std <- (vec2 - mean(vec2)) / sd(vec2)
vec2_std

mean(vec2_std); sd(vec2_std)
```
:::
:::




# Example - Exam scores {background-color="#cc0033"}

- Imagine you and a friend took exams in different classes. 
- You received the same raw score (89 out of 100)
- The exams were graded on a curve and your curved grade is higher than 
your friends. Why?

---

[Exam scores]{.emph .p-font style="font-size: 1.75em;"}

```{r}
#| label: exam-scores1
#| fig-width: 14
#| fig-height: 6

# Load test_scores data
data(test_scores)

# Calculate descriptives of class a
descriptives_a <- test_scores |> 
  filter(group == "Class_A") |> 
  group_by(group) |> 
  summarize(
    Range = max(score) - min(score), 
    Median = median(score),
    `Mean score` = mean(score), 
    `SD score` = sd(score), .groups = "drop"
  )

# Plot class a
test_scores |> 
  filter(group == 'Class_A') |>
  ggplot() + 
  aes(y = group, x = score, label = id) + 
  geom_vline(
    xintercept = descriptives_a$`Mean score`, 
    color = 'red', lty = 2, linewidth = 1.5
  ) +
  geom_vline(
    xintercept = descriptives_a$`Mean score` + descriptives_a$`SD score`, 
    color = 'darkred', lty = 2, linewidth = 1.5
  ) +
  geom_vline(
    xintercept = descriptives_a$`Mean score` - descriptives_a$`SD score`, 
    color = 'darkred', lty = 2, linewidth = 1.5
  ) +
  geom_label_repel(
    aes(fill = important), 
    nudge_y = 0.3, size = 8, color = 'black', show.legend = FALSE
  ) +
  geom_point(size = 5) + 
  scale_fill_brewer(palette = "Set1") + 
  labs(y = NULL, x = 'Score', title = NULL) + 
  theme_grey(base_family = 'Palatino', base_size = 30) + 
  theme(axis.text.y = element_text(angle = 90, hjust = 0.5))
```

. . .

```{r}
#| label: ex-descriptives1
#| results: 'asis'
descriptives_a |> 
  select(-group) |> 
  kable(format = 'html')
```

---

```{r}
#| label: exam-scores2
#| fig-width: 14
#| fig-height: 7

# Replot class a with different x lims
class_a_plot <- test_scores |> 
  filter(group == 'Class_A') |>
  ggplot() +
  aes(y = group, x = score, label = id) + 
  geom_vline(
    xintercept = descriptives_a$`Mean score`, 
    color = 'red', lty = 2, linewidth = 1.5
  ) +
  geom_vline(
    xintercept = descriptives_a$`Mean score` + descriptives_a$`SD score`, 
    color = 'darkred', lty = 2, linewidth = 1.5
  ) +
  geom_vline(
    xintercept = descriptives_a$`Mean score` - descriptives_a$`SD score`, 
    color = 'darkred', lty = 2, linewidth = 1.5
  ) +
  geom_label_repel(
    aes(fill = important), 
    nudge_y = 0.3, size = 7, color = 'black', show.legend = FALSE
  ) +
  geom_point(size = 5, pch = 21, aes(fill = important), show.legend = F) + 
  scale_x_continuous(position = "top") + 
  scale_fill_brewer(palette = "Set1") + 
  labs(y = NULL, x = 'Score', title = NULL) + 
  coord_cartesian(xlim = c(60, 100)) + 
  theme_grey(base_family = 'Palatino', base_size = 25) + 
  theme(axis.text.y = element_text(angle = 90, hjust = 0.5))

# Calculate descriptives of class b
descriptives_b <- test_scores |> 
  filter(group == 'Class_B') |>
  group_by(group) |> 
  summarize(
    Range = max(score) - min(score), 
    Median = median(score),
    `Mean score` = mean(score), 
    `SD score` = sd(score), .groups = "drop"
  )

# Plot class b on same scale as class a
class_b_plot <- test_scores |> 
  filter(group == 'Class_B') |>
  ggplot() + 
  aes(y = group, x = score, label = id) + 
  geom_vline(
    xintercept = descriptives_b$`Mean score`, 
    color = 'red', lty = 2, linewidth = 1.5
  ) +
  geom_vline(
    xintercept = descriptives_b$`Mean score` + descriptives_b$`SD score`, 
    color = 'darkred', lty = 2, linewidth = 1.5
  ) +
  geom_vline(
    xintercept = descriptives_b$`Mean score` - descriptives_b$`SD score`, 
    color = 'darkred', lty = 2, linewidth = 1.5
  ) +
  geom_label_repel(
    aes(fill = important), 
    nudge_y = 0.3, size = 7, color = 'black', show.legend = FALSE
  ) +
  geom_point(size = 5, pch = 21, aes(fill = important), show.legend = F) + 
  scale_fill_brewer(palette = 'Set1') + 
  labs(y = NULL, x = 'Score', title = NULL) + 
  coord_cartesian(xlim = c(60, 100)) + 
  theme_grey(base_family = 'Palatino', base_size = 25) + 
  theme(axis.text.y = element_text(angle = 90, hjust = 0.5))

# side by side
class_a_plot / class_b_plot
```

. . .

::: {style="font-size: 0.7em;"}
```{r} 
#| label: ex-descriptives2
#| results: 'asis'
bind_rows(descriptives_a, descriptives_b) |> 
  kable(format = 'html')
```
:::

---

[Exam scores - Standardized]{.emph .p-font style="font-size: 1.75em;"}

```{r}
#| label: exam-scores-std1
#| fig-width: 14
#| fig-height: 7

# standardize the scores of each group
scores <- test_scores |> 
  group_by(group) |> 
  mutate(`Std. score` = (score - mean(score)) / sd(score)) |> 
  ungroup() 

# Calculate descriptives by group
descriptives_std <- scores |> 
  group_by(group) |> 
  summarize(
    `Std. Mean` = mean(`Std. score`), 
    `Std. SD` = sd(`Std. score`), 
    .groups = "drop"
  )

# Plot standardized data
scores |> 
  mutate(group = factor(group, levels = c('Class_B', 'Class_A'))) |> 
  ggplot() + 
  aes(y = group, x = `Std. score`, label = id) + 
  geom_vline(
    xintercept = descriptives_std$`Std. Mean`, 
    color = 'red', lty = 2, linewidth = 1.5
  ) +
  geom_vline(
    xintercept = descriptives_std$`Std. Mean` + 
    descriptives_std$`Std. SD`, color = 'darkred', lty = 2, linewidth = 1.5
  ) +
  geom_vline(
    xintercept = descriptives_std$`Std. Mean` - 
    descriptives_std$`Std. SD`, color = 'darkred', lty = 2, linewidth = 1.5
  ) +
  geom_label_repel(
    aes(fill = important), 
    nudge_y = 0.3, size = 8, color = 'black', show.legend = FALSE
  ) +
  geom_point(size = 5, pch = 21, aes(fill = important), show.legend = F) + 
  scale_fill_brewer(palette = "Set1") + 
  labs(y = NULL, title = NULL) + 
  coord_cartesian(xlim = c(-2, 2)) + 
  theme_grey(base_family = 'Palatino', base_size = 30) + 
  theme(axis.text.y = element_text(angle = 90, hjust = 0.5))
```
---

::: columns
::: {.column style="font-size: 0.65em;"}

```{r}
#| label: ex-print1
#| results: 'asis'
scores |> 
  filter(group == 'Class_A') |> 
  select(-important, -group) |> 
  arrange(score) |> 
  mutate(`Dev. score` = `Std. score` * 6.41) |> 
  mutate_if(is.numeric, give_n_digits, n = 2) |> 
  kable(format = 'html', align = c("l", "r", "r", "r")) |> 
  row_spec(5, bold = T, color = "white", background = "#cc0033")
  
```
:::

::: {.column style="font-size: 0.65em;"}
```{r}
#| label: ex-print2
#| results: 'asis'
scores |> 
  filter(group == 'Class_B') |> 
  select(-important, -group) |> 
  arrange(score) |> 
  mutate(`Dev. score` = `Std. score` *  9.70) |> 
  mutate_if(is.numeric, give_n_digits, n = 2) |> 
  kable(format = 'html', align = c("l", "r", "r", "r")) |> 
  row_spec(10, bold = T, color = "white", background = "#cc0033")
```
:::
:::

. . .

</br>

[You're friend received the same score on her exam,<br>but you did better relative to the rest of your class]{.p-font style="font-size: 1.2em; color: #666666;"}



# Key points

::: {style="font-size: 0.8em;"}

- EDA and CDA are essential parts of data science
- Obtaining scientific data in not a trivial process 
- Observations can be classified as nominal, ordinal, interval or ratio
- Histograms of a sample distribution are useful for visualizing data
- Descriptive statistics allow us to visualize data mathematically
- There are many families of distributions
- The normal distribution is important for hypothesis testing
- QQ plots can help to determine if data are normal
- z-scores can be used to compare data derived from different sources

:::

# [References]{.emph} {.final}

::: notes
@wickham2016r
@qml_ch1
@manga2009
:::
