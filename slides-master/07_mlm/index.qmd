---
title   : 'Data Science for Linguists'
subtitle : 'Multilevel models'
author   : "Joseph V. Casillas, PhD"
institute: "Rutgers University<mybr>Spring 2025<br>Last update: `r Sys.Date()`"
---

```{r}
#| label: load-helpers
#| echo: false 
#| message: false 
#| warning: false
source(here::here("assets", "scripts", "helpers.R"))
source(here("07_mlm", "index_files", "scripts", "lmem.R"))
```

::: {style="float: right;"}

```{r}
#| label: tiktok1
#| eval: true
t1_url <- "https://www.tiktok.com/@chelseaparlettpelleriti/video/6828191481187110150"
t1 <- tiktok_embed(t1_url)
t1
```

:::


# Next steps {.transition}

---

## What we've seen

::: {style="font-size: 0.8em;"}
- MRC
- Linear regression
- General linear model
- Generalized linear model
:::

::: {.r-stack}
![](./index_files/img/lm_ex1.png){.fragment width=700 .absolute right=-50 top=10}
![](./index_files/img/lm_ex2.png){.fragment width=715 .absolute right=-60 top=10}
:::

---

## What about repeated measures designs?

::: {style="font-size: 0.85em;"}
::: {.closelist}
- Whenever we have more than one data point from the same 
participant we are dealing with a type of repeated measures 
design
  - between subjects factor
  - within subjects factor
:::

- Everything we have done this semester has been under the 
assumption that we have one data point per participant (i.e., 
no within subjects factors)

- This is because one of the assumptions of our models was that 
there was no autocorrelation, i.e., that the data were independent 

- Repeated measures designs introduce autocorrelation into the 
model. Why?
:::

---

## What's the big deal?

- Disregarding lack of independence = pseudo-replication

- In other words, replicating the data as though they were 
independent when they arenâ€™t

- This will inflate your degrees of freedom, completely bias your 
parameter estimates and make your p-values meaningless


---

## What does a repeated measures design look like?

::: {.columns}
::: {.column style="font-size: 0.62em;" width="50%"}
- This dataset has the weight of 50 different chicks at 12 
different time points
- Notice that the first 12 rows come from the same `chick` (chick 1)
- In other words, `Time` can be thought of as a continuous time series 
variable (within-subjects) that would violate our assumptions of independence
- `Diet`, on the other hand, is a between-subjects factor (you 
can't be on more than one diet at a time)
- Notice that both `Time` and `Diet` appear in this dataset as 
numeric values. 
- You have to *think* about your data to make sure you understand 
what type of variables you have.
:::

::: {.column width="10%"}
:::

::: {.column width="40%"}
```{r} 
#| label: chicks
ChickWeight |> 
  as_tibble() |> 
  slice(1:20) |> 
  kable() |> 
  kable_styling(font_size = 16)
```
:::
:::

---

## What can we do?

::: {style="font-size: 0.9em;"}
- Currently, the most popular solution is to use a multi-level or 
hierarchical model
- These models are commonly referred to as mixed effects models 
- They include both "fixed" effects and "random" effects
- They provide a flexible framework that allows us to account for 
the hierarchical structure of our data (within-subjects variables, 
time-series data, etc.) and provide *partial pooling* estimates
- Building on our knowledge of the linear model, understanding 
the basic idea of mixed effects models is trivial
:::

---

## How does it work?

### Standard linear model

::: {style="font-size: 0.72em;"}
- Recall our formula for fitting a linear model

::: {.fragment}
$$
\begin{align}
y_{i}   & \sim Normal(\mu_{i}, \sigma) \\
\mu_{i} & = \alpha + \beta_{1} x_{i} \\
\sigma  & \sim Normal(0, \sigma^{2})
\end{align}
$$
:::

::: {.fragment}
- We fit these models in R using `lm()` or `glm()`:  
`lm(criterion ~ predictor, data = my_data)`
- In a mixed effects model, what we know as *predictors* are called **[fixed effects]{.emph}**
- The novel aspect of a mixed effects model is that it also includes [random effects]{color="blue"}
- Random effects allow us to account for non-independence
:::
:::

---

## How does it work? 

### Mixed effects model

::: {style="font-size: 0.85em;"}
- A mixed effects model *mixes* both fixed effects and random effects

::: {.fragment}
$$
\hat{y} = \alpha + \color{red}{\beta}\color{blue}{X} + \color{green}{u}\color{purple}{Z} + \epsilon
$$
:::

::: {.fragment}
$$
response = intercept + \color{red}{slope} \times \color{blue}{FE} + \color{green}{u} \times \color{purple}{RE} + error
$$
:::

::: {.fragment}
- We fit these models in R using `lmer()` or `glmer()` from the `lme4` package (there are other options as well)

```
lmer(criterion ~ fixed_effect + (1|random_effect), data = my_data)
```
:::
:::

---

## What is a random effect? 

::: {style="font-size: 0.75em;" .closelist}
- This is actually a really nuanced question and nobody has a good 
answer
- Some people use descriptions like the following:
:::

<p></p>

::: {style="font-size: 0.75em;" .closelist}
| fixed                     | random                 |
| :------------------------ | :--------------------- |
| repeatable                | non repeatable         |
| systematic influence      | random influence       |
| exhaust the pop.          | sample the pop.        |
| generally of interest     | often not of interest  |
| continuous or categorical | have to be categorical |

<p></p>

- ...but you will always find exceptions to this
- Most of the time you will see **[subjects]{.emph}** and **[items]{.emph}** as random effects (things that are repeated in the design)
- This typically implies that the model includes **[random intercepts]{.emph}**, and/or [random slopes]{color="blue"}
- Instead of trying to define random effects, let's try to understand what they do (and come back to the idea later)
:::

---

## {.center}

```{r}
#| label: lmem_plot_raw
my_df |> 
  ggplot() + 
  aes(x = time, y = response) + 
  geom_point(pch = 21, color = "white", fill = "#cc0033", size = 3) + 
  labs(y = "Response", x = "Time") + 
  ds4ling_bw_theme(base_size = 22) + 
  theme(plot.margin = margin(0, 0, 0, 0, "cm"))
```

---

## {.center}

::: {.columns}
::: {.column style="font-size: 0.5em; width: 40%"}
```{r}
#| label: lmem_structure
my_df |> 
  select(subjects, time, response) |> 
  as_tibble() |> 
  mutate(response = round(response, 2)) |> 
  head(24) |> 
  kable(format = 'html')
```
:::

::: {.column width="10%"}
:::

::: {.column style="font-size: 0.7em; width: 50%"}

<br>

- The dataframe includes values of the `response` variable at 20 
different time points for each participant (n = 10)
- We could model this as:

```
lm(response ~ time, data = my_df)
```

$$
\begin{align}
response_{i} & \sim N(\mu_{i}, \sigma) \\
\mu_{i}      & = \alpha + \beta_1 time_{i} \\
\sigma       & \sim Normal(0, \sigma^{2})
\end{align}
$$

- But this would violate our assumption of independence (and would 
produce a terribly misspecified model)
:::
:::

---

## {.center}

```{r}
#| label: lmem_plot_lm
my_df |> 
  ggplot() + 
  aes(x = time, y = response) + 
  geom_point(pch = 21, color = "white", fill = "#cc0033", size = 3) + 
  geom_smooth(method = lm, formula = "y ~ x") + 
  labs(y = "Response", x = "Time") + 
  ds4ling_bw_theme(base_size = 22) + 
  theme(plot.margin = margin(0, 0, 0, 0, "cm"))
```

---

## Let's include `subjects` as a random effect

<br>

::: {.columns}
::: {.column style="font-size: 0.7em; width: 55%"}
- We will do this by giving subjects a random intercept
- In other words, we will allow the intercepts to vary for each 
participant (as opposed to modeling just 1 intercept)

```
lmer(response ~ time + (1|subjects), data = my_df)
```

- What would this look like?
:::

::: {.column width="3%"}
:::

::: {.column .fragment style="font-size: 0.8em; width: 42%;"}

<br>

$$
\begin{align}
response_{it} & \sim N(\mu_{it}, \sigma) \\
\mu_{it}      & = \alpha + \alpha_{subject_i} + \beta_1 time_{t} \\
\sigma        & \sim Normal(0, \sigma^{2})
\end{align}
$$
:::
:::

---

## {.center}

```{r}
#| label: lmem_plot_subj
order <- c("p_01", "p_02", "p_03", "p_04", "p_05", "p_06", "p_07", "p_08", "p_09", "p_10")

my_df |> 
  mutate(subjects = fct_relevel(subjects, order)) |> 
  ggplot() + 
  aes(x = time, y = response, fill = subjects) + 
  geom_point(pch = 21, color = "white", size = 3) + 
  labs(y = "Response", x = "Time") + 
  scale_fill_viridis_d(name = NULL, option = "C", end = 0.9) + 
  ds4ling_bw_theme(base_size = 22) + 
  theme(plot.margin = margin(0, 0, 0, 0, "cm"))
```

---

## {.center}

```{r}
#| label: lmem_plot_ranInt
my_df |> 
  mutate(subjects = fct_relevel(subjects, order)) |> 
  ggplot() +  
  aes(x = time, y = response, color = subjects, fill = subjects) + 
  geom_point(pch = 21, color = "white", size = 3) + 
  geom_abline(
    data = mutate(ran_ints, subjects = fct_relevel(subjects, order)), 
    mapping = aes(intercept = intercepts, slope = slopes, color = subjects), 
    linewidth = 0.8, show.legend = F,  
  ) + 
  geom_abline(
    intercept = fixef(mod_int)[[1]], 
    slope = fixef(mod_int)[[2]], 
    color = "darkgrey", linewidth = 2
  ) + 
  scale_fill_viridis_d(name = NULL, option = "C", end = 0.9) + 
  scale_color_viridis_d(name = NULL, option = "C", end = 0.9) + 
  labs(y = "Response", x = "Time") + 
  ds4ling_bw_theme(base_size = 22) + 
  theme(plot.margin = margin(0, 0, 0, 0, "cm"))
```

---

## What did we do? {background-image="../assets/img/pensar2.png" background-position="90% 50%" background-size="500px"}

::: {.columns}
::: {.column style="font-size: 0.65em; width: 60%"}
- We are taking into account the idiosyncratic differences 
associated with each individual subject
- By giving each subject its own intercept we are informing the 
model that each individual has a different starting point in 
the time course (when `time` = 0)

::: {.closelist}
- In general this makes sense because some people...
  - are faster/slower responders
  - speak faster/slower
  - have higher/lower pitched voices
:::

<p></p>

::: {.closelist}
- We can also take into account the fact that some people...
  - slow down during an experiment
  - respond more/less accurately over time
  - learn at different rates
:::
:::
:::

---

## {.center}

```{r}
#| label: lmem_plot_ranSlopes
my_df |> 
  mutate(subjects = fct_relevel(subjects, order)) |> 
  ggplot() + 
  aes(x = time, y = response, fill = subjects) + 
  geom_point(pch = 21, color = "white", size = 3) + 
  geom_smooth(
    aes(color = subjects), 
    method = lm, se = F, formula = "y ~ x"
  ) + 
  scale_fill_viridis_d(name = NULL, option = "C", end = 0.9) + 
  scale_color_viridis_d(name = NULL, option = "C", end = 0.9) + 
  labs(y = "Response", x = "Time") + 
  ds4ling_bw_theme(base_size = 22) +
  theme(plot.margin = margin(0, 0, 0, 0, "cm"))
```

---

## What did we do? {background-image="../assets/img/pensar2.png" background-position="90% 50%" background-size="500px"}

::: {.columns}
::: {.column style="font-size: 0.7em; width: 60%"}
- The model now allows the random intercepts to vary for each 
individual for the effect `time`
- This means we included a random slope for each participant
- By adding a random slope for the effect `time` we take into 
account the fact that `response` change for each individual at 
a different rate
- Under the hood, the model uses this information to calculate 
the best fit line for all of the data
- This method is called partial pooling and represents one of 
the most important (and least understood) aspects of mixed 
effects modeling
:::
:::

---

## {.center}

::: {.columns}
::: {.column width="50%"}

::: {style="font-size: 0.65em;"}
```
lmer(response ~ time + (1 + time|subjects), data = my_df)
```
:::

::: {style="font-size: 0.7em;"}
- Again, `(1 + time|subjects)` represents the random structure 
of the model
- Anything to the right of `|` is a random intercept
- Anything to the left of `|` is given a random slope for the 
effect specified to the right
- Thus, `(1 + time|subjects)` means random slopes for the effect `time` for each subject
- It is rarely a good idea to only use random intercepts
:::
:::

::: {.column wdith="50%"}

<br><br>

```{r}
#| label: lmem_plot_ranSlopes2
#| fig-asp: 0.8
my_df |> 
  ggplot() + 
  aes(x = time, y = response, group = subjects) + 
  geom_point(
    aes(color = subjects), 
    pch = 21, fill = 'grey60', size = 3, show.legend = F
  ) + 
  geom_smooth(method = lm, se = F, color = 'grey60', formula = "y ~ x") + 
  geom_abline(
    intercept = fixef(mod_slp)[1], 
    slope = fixef(mod_slp)[2], 
    color = 'darkred', size = 2
  ) + 
  labs(y = "Response", x = "Time") + 
  ds4ling_bw_theme(base_size = 24) + 
  theme(plot.margin = margin(0, 0, 0, 0, "cm"))
```
:::
:::

---

## Great news!

::: {.columns}
::: {.column width="50%"}
::: {style="font-size: 0.6em;" .closelist}
- We have spent the entire semester building up our knowledge 
of the linear model so that we would be prepared to understand 
mixed effects models 
- Why? They are the standard in speech sciences now

<p></p>

- Everything that you have learned in this class applies 
to a mixed effects model 
  - model interpretation
  - nested model comparisons
  - treatment of categorical factors
  - centering, standardizing, and other transformations

<p></p>

- What about GLMs?
  - Them too!
  - You can fit mixed effects models for count data and 
  binary outcomes using `glmer()`
  - All you have to do is specify the distribution and the 
  linking function
:::
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
::: {style="font-size: 0.8em;"}
[Multilevel logistic regession model]{style="font-size: 0.8em;"}
```r
glmer(
  formula = response ~ fixed_effect + 
    (1 + fixed_effect | participant), 
  family = binomial(link = "logit"), 
  data = DATA
)
```

<br>

[Multilevel poisson regression model]{style="font-size: 0.8em;"}
```r
glmer(
  formula = counts   ~ fixed_effect + 
    (1 + fixed_effect | participant), 
  family = poisson(link = "log"), 
  data = DATA
)
```
:::
:::
:::

---

## {background-color="#000" background-image="https://www.jvcasillas.com/media/rstats/memes/lm_mem.png" background-size="contain"}

---

## {background-color="#000" background-image="https://www.jvcasillas.com/media/rstats/memes/lm_glm_glmm_cats.png" background-size="contain"}

---

## {data-menu-title="Shiny app: GLM" background-iframe="https://mfviz.com/hierarchical-models/" background-interactive=TRUE}

<!-- Multilevel models --> 

::: footer
<https://mfviz.com/hierarchical-models/>
:::





# Understanding partial pooling {.transition}

::: {style="font-size: 0.55em;"}
Based on @bates2011mixed and @mahr2017plotting with some help from @mcelreath2018statistical
:::

---

## What do you mean by 'multiple levels'? {background-image="./index_files/img/russian_dolls.png" background-size="400px" background-position="95% 20%"}

We can think of multilevel models as <mybr>"[models within models]{.emph}"

. . .

::: {style="font-size: 0.7em;" .closelist}
<br>
Generally:

- One level estimates observed groups/items/individuals/etc.
- Another level estimates populations of groups/individuals

::: {.columns}
::: {.column .fragment}
::: box-note
For this reason, some reasercheres (myself included) prefer to refer to **grouping-level** effects and **population-level** effects, as opposed to *random* effects and *fixed* effects
:::
:::

::: {.column .fragment}

<br>

::: box-warning

<p></p>

Under this view, *random* intercepts and *random* slopes are conceptualized and **varying** intercepts and **varying** slopes

<p></p>

:::
:::
:::
:::

---

## So what is 'pooling' all about? {background-color="#000" background-image="./index_files/img/memoryx.png" background-size="750px" background-position="100% 100%"}

::: {.columns}
::: {.column style="font-size: 0.75em;"}
- Some describe multilevel models as '[**models with memory**]{.emph}'
  - The population-level model helps inform/learn about the grouping-level
  - They learn faster, better
  - They are robust to overfitting

- It has to do with how information is 'pooled'
  1. Complete pooling
  2. No pooling
  3. Partial pooling
:::
:::

---

## Partial pooling

### `sleepstudy` dataset

::: {.columns}
::: {.column style="font-size: 0.9em;"}
- Part of `lme4` package
- Criterion = reaction time (`Reaction`)
- Predictor = \# of days of sleep deprivation (`Days`)
- 10 observations per participant
- \+2 fake participants w/ incomplete data
:::

::: {.column style="font-size: 0.75em;"}

```{r}
#| label: ss-cleanup
# Convert to tibble for better printing. 
# Convert factors to strings
sleepstudy <- sleepstudy |> 
  as_tibble() |> 
  mutate(Subject = as.character(Subject))

# Add two fake participants
df_sleep <- bind_rows(
    sleepstudy,
    tibble(Reaction = c(286, 288), Days = 0:1, Subject = "374"),
    tibble(Reaction = 245, Days = 0, Subject = "373")
  ) |> 
  mutate(is_extra = if_else(Subject %in% c("373", "374"), TRUE, FALSE))

```

```{r}
#| label: ss-add-subjs
str(df_sleep)

df_sleep |> 
  slice(1:12) |> 
  kable() |> 
  kable_styling(font_size = 18)
```
:::
:::

---

## {.center}

```{r}
#| label: ss-plot0
#| fig.asp: 0.6
df_sleep |> 
  ggplot() + 
  aes(x = Days, y = Reaction) + 
  geom_point(
    aes(fill = Subject, shape = is_extra), 
    size = 3, color = "white"
  ) + 
  scale_x_continuous(breaks = 0:4 * 2) + 
  scale_fill_viridis_d(
    name = NULL, end = 0.9, direction = -1, 
    guide = guide_legend(override.aes = list(shape = 21))
  ) + 
  scale_shape_manual(name = NULL, values = c(21, 23), guide = "none") + 
  ds4ling_bw_theme(base_size = 14) + 
  theme(
    plot.margin = margin(1, 0, 0, 0, "cm"), 
    legend.key.size = unit(0.5, "cm")
  )
```

---

## {.center}

```{r}
#| label: ss-plot1
#| fig-asp: 0.6

df_no_pooling <- lmList(Reaction ~ Days | Subject, df_sleep) |> 
  coef() |> 
  tibble::rownames_to_column("Subject") |> 
  rename(Intercept = `(Intercept)`, Slope_Days = Days) |> 
  mutate(
    Model = "No pooling", 
    is_extra = if_else(Subject %in% c("373", "374"), TRUE, FALSE)
  ) |> 
  filter(Subject != "373") |> 
  select(Model, everything())

xlab <- "Days of sleep deprivation"
ylab <- "Average reaction time (ms)"

df_sleep |> 
  ggplot() + 
  aes(x = Days, y = Reaction) + 
  geom_abline(
    data = df_no_pooling, show.legend = F, 
    aes(intercept = Intercept, slope = Slope_Days, color = is_extra)
  ) + 
  geom_point(
    aes(fill = is_extra), 
    size = 3, color = "white", shape = 21, show.legend = F
  ) +
  facet_wrap("Subject") +
  labs(
    x = xlab, y = ylab, 
    title = "No-pooling", 
    subtitle = "Info from each participant is not pooled together, i.e., there is an individual model for each person."
  ) + 
  scale_x_continuous(breaks = 0:4 * 2) + 
  scale_fill_manual(values = c("#000", "#cc0033")) + 
  scale_color_manual(values = c("#000", "#cc0033")) + 
  ds4ling_bw_theme(base_size = 12) + 
  theme(plot.margin = margin(0, 0, 0, 0, "cm"))
```

---

## No pooling

```{r}
#| label: no-pooling
df_no_pooling |> 
  select(-is_extra) |> 
  kable() |> 
  kable_styling(font_size = 18)
```

::: notes
This is the same as fitting a separate line for each cluster of data

The models are unaware that any of the other participants exist. 
:::

---

## {.center}

```{r}
#| label: ss-plot2
# Fit a model on all the data pooled together
m_pooled <- lm(Reaction ~ Days, df_sleep) 

# Repeat the intercept and slope terms for each participant
df_pooled <- tibble(
  Model = "Complete pooling",
  Subject = unique(df_sleep$Subject),
  Intercept = coef(m_pooled)[1], 
  Slope_Days = coef(m_pooled)[2]
)

df_sleep |> 
  ggplot() + 
  aes(x = Days, y = Reaction) + 
  geom_point(pch = 21, size = 3, color = "white", fill = "#cc0033") + 
  geom_smooth(method = lm, formula = "y ~ x") + 
  scale_x_continuous(breaks = 0:4 * 2) + 
  labs(x = xlab, y = ylab, 
    title = "Complete pooling", 
    subtitle = "RT ~ Days, model is unaware of participant clusters.") + 
  ds4ling_bw_theme(base_size = 14) + 
  theme(plot.margin = margin(0, 0, 0, 0, "cm"))
```

---

## Complete pooling

```{r}
#| label: complete-pooling-lm
summary(m_pooled)
```

---

## Complete pooling

```{r}
#| label: complete-pooling-tbl
df_pooled |> 
  kable() |> 
  kable_styling(font_size = 17)
```

---

## {.center}

```{r}
#| label: ss-plot3
#| fig-asp: 0.6
# Join the raw data so we can use plot the points and the lines.
df_models <- bind_rows(df_pooled, df_no_pooling) |> 
  left_join(df_sleep, by = "Subject", relationship = "many-to-many")

df_models |> 
  ggplot() + 
  aes(x = Days, y = Reaction) + 
  geom_abline(
    aes(intercept = Intercept, slope = Slope_Days, color = Model),
    linewidth = .75
  ) + 
  geom_point(pch = 21, color = "white", fill = "#000", size = 3) +
  facet_wrap("Subject") +
  labs(x = xlab, y = ylab, 
    title = "Comparison", 
    subtitle = "Complete pooling vs. no pooling.") + 
  scale_x_continuous(breaks = 0:4 * 2) + 
  scale_color_manual(values = c("#cc0033", "#000")) + 
  ds4ling_bw_theme(base_size = 12) + 
  theme(
    plot.margin = margin(0, 0, 0, 0, "cm"), 
    legend.position = "bottom", legend.justification = "left", 
    legend.margin = margin(-15, 0, 0, 0),
    legend.box.margin = margin(0, 0, 0, 0)
  )
```

::: notes
Complete pooling is the same line for each subject, sometimes it is a good fit, sometimes it is way off (i.e., 309)
:::

---

## {.center}

```{r}
#| label: ss-plot4
df_models |> 
  filter(Subject %in% c("308", "309", "333", "335", "373", "374")) |> 
  ggplot() + 
  aes(x = Days, y = Reaction) + 
  geom_abline(
    aes(intercept = Intercept, slope = Slope_Days, color = Model),
    linewidth = 0.75
  ) + 
  geom_point(pch = 21, color = "#fff", fill = "#000", size = 4) +
  facet_wrap("Subject") +
  labs(x = xlab, y = ylab, 
    title = "Comparison", 
    subtitle = "Complete pooling vs. no pooling.") + 
  scale_x_continuous(breaks = 0:4 * 2) + 
  scale_color_manual(values = c("#cc0033", "#000")) + 
  ds4ling_bw_theme(base_size = 14) + 
  theme(
    plot.margin = margin(0, 0, 0, 0, "cm"), 
    legend.position = "bottom", legend.justification = "left"
  )
```

::: notes
Complete pooling is the same line for each subject, sometimes it is a good fit, sometimes it is way off (i.e., 309)

subj 373 is interesting... when you know very little, best guess is complete pooling estimate

no pooling model follows the data, steep slope for 308, negative slope for 335.
but... moving from one cluster (subj) to another, they forget everything about the previous clusters (no pooling = model with amnesia)
:::

---

## [Can we improve estimates with partial pooling?]{.emph} {.center background-color="black"}

::: notes
The mixed effects model will pool information from all the lines to improve estimates of each individual line. 

after seeing data from participants with complete data, the model makes an informed guess about the lines for the participants with incomplete data
:::

---

## {.center}

<center>
`Reaction ~ 1 + Days + (1 + Days | Subject)`
</center>

<br>

. . .

- We allow the effect of `Days` to vary for each `Subject`

. . .

- By-subject random intercepts with random slope for `Days`

```r
lmer(Reaction ~ 1 + Days + (1 + Days | Subject), data = df_sleep)
```

---

## {.center style="font-size: 0.7em;"}

::: {.columns}
::: {.column}
```{r}
#| label: lmem
m <- lmer(Reaction ~ 1 + Days + (1 + Days | Subject), df_sleep)
summary(m)
```
:::

::: {.column}
- Most of this will look familiar
- Fixed effects estimates (and interpretations) work just like lm, glm, etc. 
- What's new? The random effects estimates
:::
:::

---

## Partial pooling

```{r}
#| label: lmem-re
# Make a dataframe with the fitted effects
df_partial_pooling <- coef(m)[["Subject"]] |> 
  tibble::rownames_to_column("Subject") |> 
  as_tibble() |> 
  rename(Intercept = `(Intercept)`, Slope_Days = Days) |> 
  mutate(Model = "Partial pooling")

df_partial_pooling |> 
  kable() |> 
  kable_styling(font_size = 18)
```

---

## {.center}

```{r}
#| label: plot-all-pooling-types
df_models <- bind_rows(
  df_pooled, df_no_pooling, df_partial_pooling) |>
  left_join(df_sleep, by = "Subject", relationship = "many-to-many")

df_models |> 
  ggplot() + 
  aes(x = Days, y = Reaction) + 
  geom_abline(
    aes(intercept = Intercept, slope = Slope_Days, color = Model),
    linewidth = .75
  ) + 
  geom_point(pch = 21, color = "white", fill = "#000", size = 3) +
  facet_wrap("Subject") +
  labs(
    y = ylab, x = xlab, 
    title = "Comparison", 
    subtitle = "No pooling vs. Complete pooling vs. Partial pooling"
  ) + 
  scale_x_continuous(breaks = 0:4 * 2) + 
  scale_color_manual(values = c("#cc0033", "#000", "green")) + 
  ds4ling_bw_theme(base_size = 12) + 
  theme(
    plot.margin = margin(0, 0, 0, 0, "cm"), 
    legend.position = "bottom", legend.justification = "left", 
    legend.margin = margin(-15, 0, 0, 0),
    legend.box.margin = margin(0, 0, 0, 0)
  )
```

---

## {.center}

```{r}
#| label: plot-all-pooling-types-subset

df_models |> 
  filter(Subject %in% c("308", "309", "333", "335", "373", "374")) |> 
  ggplot() + 
  aes(x = Days, y = Reaction) + 
  geom_abline(
    aes(intercept = Intercept, slope = Slope_Days, color = Model),
    linewidth = 0.75
  ) + 
  geom_point(pch = 21, color = "#fff", fill = "#000", size = 4) +
  facet_wrap("Subject") +
  labs(
    y = ylab, x = xlab, 
    title = "Comparison", 
    subtitle = "No pooling vs. Complete pooling vs. Partial pooling"
  ) + 
  scale_x_continuous(breaks = 0:4 * 2) + 
  scale_color_manual(values = c("#cc0033", "#000", "green")) + 
  ds4ling_bw_theme(base_size = 14) + 
  theme(
    plot.margin = margin(0, 0, 0, 0, "cm"), 
    legend.position = "bottom", legend.justification = "left", 
    legend.margin = margin(-15, 0, 0, 0),
    legend.box.margin = margin(0, 0, 0, 0)
  )
```

::: notes
no pooling and partial pooling estimates are generally similar

if they differ, the partial pooling model estimate is pulled slightly towards the complete-pooling line (shrinkage).
:::

---

## {background-color="#000" background-image="https://i.pinimg.com/originals/65/8b/7c/658b7c3e1ace3cb6fc6bf7a6fa7748b9.jpg" background-size="contain"}

<!-- costanza shirnkage meme -->

---

## {data-menu-title="Shiny app: Parameter space" background-iframe="https://www.jvcasillas.com/shiny_parameter_space/" background-interactive=TRUE}

<!-- Parameter space --> 

::: footer
<https://www.jvcasillas.com/shiny_parameter_space/>
:::

---

## {background-color="black"}

```{r}
#| label: topo-map
#| fig-asp: 0.6
# Also visualize the point for the fixed effects
df_fixef <- tibble(
  Model = "Partial pooling (average)",
  Intercept = fixef(m)[1],
  Slope_Days = fixef(m)[2]
)

# Complete pooling / fixed effects are center of gravity in the plot
df_gravity <- df_pooled |> 
  distinct(Model, Intercept, Slope_Days) |> 
  bind_rows(df_fixef)

df_pulled <- bind_rows(df_no_pooling, df_partial_pooling)

# Extract the matrix
cov_mat <- VarCorr(m)[["Subject"]]

# Strip off some details so that just the useful part is printed
attr(cov_mat, "stddev") <- NULL
attr(cov_mat, "correlation") <- NULL

# Helper function to make a data-frame of ellipse points that 
# includes the level as a column
make_ellipse <- function(cov_mat, center, level) {
  ellipse::ellipse(cov_mat, centre = center, level = level) |>
    as.data.frame() |>
    mutate(level = level) |> 
    as_tibble()
}

center <- fixef(m)
levels <- c(.1, .3, .5, .7, .9)

df_ellipse <- levels |>
  purrr::map_df(~ make_ellipse(cov_mat, center, level = .x)) |> 
  rename(Intercept = `(Intercept)`, Slope_Days = Days)

# Euclidean distance
contour_dist <- function(xs, ys, center_x, center_y) {
  x_diff <- (center_x - xs) ^ 2
  y_diff <- (center_y - ys) ^ 2
  sqrt(x_diff + y_diff)
}

# Find the point to label in each ellipse.
df_label_locations <- df_ellipse |> 
  group_by(level) |>
  filter(
    Intercept < quantile(Intercept, 0.25), 
    Slope_Days < quantile(Slope_Days, 0.25)
  ) |> 
  # Compute distance from center.
  mutate(dist = contour_dist(
    Intercept, Slope_Days, fixef(m)[1], fixef(m)[2])) |>
  # Keep smallest values.
  top_n(-1, wt = dist) |> 
  ungroup()

ggplot(df_pulled) + 
  aes(x = Intercept, y = Slope_Days, color = Model) + 
  geom_path(aes(group = level, color = NULL), data = df_ellipse,
    linetype = "dashed", color = "grey74") +
  geom_point(data = df_gravity, size = 9, pch = 20:21, stroke = 3) + 
  geom_point(size = 4) + 
  geom_path(aes(group = Subject, color = NULL), 
    arrow = arrow(length = unit(.02, "npc"))) + 
  geom_text(aes(label = level, color = NULL), 
    data = df_label_locations, nudge_x = .5, nudge_y = .8, 
    size = 3.5, color = "grey60") + 
  labs(title = NULL, 
    subtitle = "Topographic map of regression parameters", 
    x = "Intercept estimate", y = "Slope estimate") +
  scale_color_viridis_d(option = "C", begin = 0.3) + 
  ggdark::dark_theme_gray(base_size = 18, base_family = "Palatino") + 
  theme(
    plot.margin = margin(0, 0, 0, 0, "cm"), 
    legend.position = "bottom", 
    legend.justification = "left"
  ) 
```

---

## {background-color="#000" .center}

```{r}
#| label: partial-pooling-summary
#| fig-asp: 0.6
# From : https://www.tjmahr.com/another-mixed-effects-model-visualization/

# Select four participants to highlight
df_demo <- df_sleep |>
  dplyr::filter(Subject %in% c("335", "333", "350", "374"))

sleep_bda_mod <- brms::brm(
  Reaction ~ Days + (Days | Subject),
  data = df_sleep, 
  seed = 20191125, 
  file = here("assets", "mods", "sleep_mod")
)

col_data <- "grey80"
tag_others <- "   (others)   "

p_top <- ggplot(df_demo) +
  aes(x = Days, y = Reaction) +
  geom_line(aes(group = Subject), color = col_data) +
  geom_point(color = col_data, size = 2) +
  facet_wrap("Subject", nrow = 1) +
  scale_x_continuous(breaks = seq(0, 9, by = 2)) +
  coord_cartesian(ylim = c(200, 500)) +
  ggtitle("Each participant contributes data") +
  labs(x = NULL, y = "") + 
  ggdark::dark_theme_gray(base_size = 14, base_family = "Palatino") +
  theme(
    plot.margin = margin(0, 0, 0, 0, "cm"), 
    strip.background = element_blank(),
    strip.text = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid.minor = element_blank(),
    plot.tag.position = "right",
    plot.tag = element_text(size = rel(.9))
    #plot.title.position = "plot"
  )

df_demo_fitted <- df_demo |>
  # Create a dataframe with all possible combination of Subject and Days
  tidyr::expand(
    Subject,
    Days = seq(min(Days), max(Days), by = 1)
  ) |>
  # Get the posterior predictions
  tidybayes::add_epred_draws(sleep_bda_mod)

col_data <- "grey80"
tag_others <- "   (others)   "

p_bottom <- ggplot(df_demo_fitted) +
  aes(x = Days, y = .epred) +
  # .width is the interval width
  tidybayes::stat_lineribbon(alpha = .4, .width = .95, 
    color = "white") +
  geom_point(
    aes(y = Reaction), 
    data = df_demo, 
    size = 2, 
    color = col_data
  ) +
  facet_wrap("Subject", nrow = 1) +
  # Use the viridis scale on the ribbon fill
  scale_color_viridis_d(option = "A", aesthetics = "fill", 
    begin = 0.5) +
  # No legend
  guides(fill = FALSE) +
  scale_x_continuous(breaks = seq(0, 9, by = 2)) +
  coord_cartesian(ylim = c(200, 500)) +
  ggtitle(
    "Individual trajectories \"borrow information\" from others"
  ) +
  labs(x = NULL, y = "") + 
  ggdark::dark_theme_gray(base_size = 14, base_family = "Palatino") +
  theme(
    plot.margin = margin(0, 0, 0, 0, "cm"), 
    strip.background = element_blank(),
    strip.text = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid.minor = element_blank(),
    plot.tag.position = "right",
    plot.tag = element_text(size = rel(.9))
    #plot.title.position = "plot"
  )

df_population <- df_sleep |>
  distinct(Days) |>
  mutate(Subject = "fake") |>
  tidybayes::add_epred_draws(sleep_bda_mod, allow_new_levels = TRUE)

p_population <- ggplot(df_population) +
  aes(x = Days, y = .epred) +
  tidybayes::stat_lineribbon(
    # new part
    color = "#11111100",
    .width = c(.1, .25, .5, .6, .7, .8, .9, .95)
  ) +
  scale_x_continuous(
    "Days",
    breaks = seq(0, 9, by = 2),
    minor_breaks = NULL
  ) +
  coord_cartesian(ylim = c(200, 500)) +
  scale_y_continuous("Reaction Time") +
  scale_color_viridis_d(option = "A", aesthetics = "fill") +
  guides(fill = FALSE) +
  ggtitle("Model estimates the population of participants") +
  ggdark::dark_theme_gray(base_size = 14, base_family = "Palatino") + 
  theme(plot.margin = margin(0.2, 0, 0.2, 0, "cm"))

p_middle <- cowplot::plot_grid(
  p_population, 
  NULL, 
  nrow = 1, 
  rel_widths = c(3, 0)
)

cowplot::plot_grid(
  p_top,
  p_middle,
  p_bottom,
  ncol = 1,
  rel_heights = c(1, 2, 1)
)
```

---

## {background-color="black" .center}

::: {.columns}
::: {.column style="font-size: 0.75em; width: 40%"}

- Multilevel models use the grouping variables to learn about the nested structure of the data
- The model can use this information to make educated guesses when there is incomplete information
- The model takes advantage of partial pooling, producing shrinkage
- This builds skepticism into the model with regard to extreme values 
:::

::: {.column width="2%"}
:::

::: {.column width="58%"}

```{r}
#| label: partial-pooling-summary2
#| fig-asp: 1.1

cowplot::plot_grid(
  p_top,
  p_middle,
  p_bottom,
  ncol = 1,
  rel_heights = c(1, 2, 1)
)
```
:::
:::





# [References]{.emph} {.final visibility="uncounted"}

::: {#refs}
:::

::: notes
@bates2011mixed
@mahr2017plotting
@mahr2019another
@brown2021introduction
@debruine2021understanding
:::